"""
College Crawler ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""

import argparse
import json
import os
import sys
import uuid
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Optional

sys.path.insert(0, str(Path(__file__).parent.parent))

from sqlalchemy import text

from src.crawlers.school_crawler import SchoolCrawler
from src.database.connection import get_db
from src.database.models import AuditLog, School
from src.database.repository import SchoolRepository
from src.services.scorecard_enrichment_service import ScorecardEnrichmentService
from src.utils.failed_sites import failed_site_manager
from src.utils.logger import setup_logger

logger = setup_logger(__name__)
SYSTEM_ACTOR_ID = uuid.UUID("00000000-0000-0000-0000-000000000000")
_AUDIT_USER_ID_CACHE: Optional[uuid.UUID] = None
_SCORECARD_SERVICE = ScorecardEnrichmentService()


def load_schools_list(json_file: Path) -> list:
    """
    í•™êµ ëª©ë¡ JSON íŒŒì¼ ë¡œë“œ
    
    Args:
        json_file: JSON íŒŒì¼ ê²½ë¡œ
        
    Returns:
        í•™êµ ëª©ë¡
    """
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get('schools', [])


def _find_school_record(db, name: str, website: str) -> Optional[School]:
    """ì´ë¦„/ì›¹ì‚¬ì´íŠ¸ë¡œ í•™êµ ë ˆì½”ë“œë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    school = db.query(School).filter(
        School.name == name,
        School.website == website,
    ).first()
    if school:
        return school

    return db.query(School).filter(School.name == name).first()


def _resolve_audit_user_id(db) -> Optional[uuid.UUID]:
    """
    ìš´ì˜ DBì—ì„œ audit_logs.user_id/created_by/updated_byê°€ NOT NULL(ë° FK)ì¼ ìˆ˜ ìˆì–´
    "ì‹¤ì¡´í•˜ëŠ” users.id"ë¥¼ í•´ì„í•©ë‹ˆë‹¤.

    ìš°ì„ ìˆœìœ„:
    1) í™˜ê²½ë³€ìˆ˜ AUDIT_SYSTEM_USER_ID ë˜ëŠ” AUDIT_USER_ID (UUID)
    2) users í…Œì´ë¸”ì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ 1ê°œ(id) ì„ íƒ
    """
    global _AUDIT_USER_ID_CACHE
    if _AUDIT_USER_ID_CACHE:
        return _AUDIT_USER_ID_CACHE

    env_value = os.getenv("AUDIT_SYSTEM_USER_ID") or os.getenv("AUDIT_USER_ID")
    if env_value:
        try:
            _AUDIT_USER_ID_CACHE = uuid.UUID(env_value.strip())
            return _AUDIT_USER_ID_CACHE
        except ValueError:
            logger.warning(f"AUDIT_SYSTEM_USER_ID/AUDIT_USER_ID í˜•ì‹ ì˜¤ë¥˜(ë¬´ì‹œ): {env_value!r}")

    try:
        row = db.execute(text("SELECT id FROM users ORDER BY created_at ASC LIMIT 1")).fetchone()
        if row and row[0]:
            _AUDIT_USER_ID_CACHE = uuid.UUID(str(row[0]))
            return _AUDIT_USER_ID_CACHE
    except Exception as e:
        logger.warning(f"audit user_id ìë™ í•´ì„ ì‹¤íŒ¨(ë¬´ì‹œ): {e}")

    return None


def _update_school_crawl_metadata(name: str, website: str, status: str, message: str) -> None:
    """schools ìµœì‹  í¬ë¡¤ë§ ìƒíƒœ ì»¬ëŸ¼ì„ ê°±ì‹ í•©ë‹ˆë‹¤(ì‹¤íŒ¨/ìŠ¤í‚µ í¬í•¨)."""
    try:
        with get_db() as db:
            school = _find_school_record(db, name, website)
            if not school:
                return
            now = datetime.now(timezone.utc)
            school.last_crawled_at = now
            school.last_crawl_status = status
            school.last_crawl_message = message
            db.flush()
    except Exception as e:
        logger.warning(f"schools í¬ë¡¤ë§ ë©”íƒ€ë°ì´í„° ê°±ì‹  ì‹¤íŒ¨(ë¬´ì‹œ): {name} - {e}")


def _build_school_payload(
    name: str,
    website: str,
    crawled_data: Dict[str, Any],
    seed_school: Optional[Dict[str, Any]],
    existing_school: Optional[School],
) -> Dict[str, Any]:
    """DB ì €ì¥ìš© í•™êµ payloadë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    payload: Dict[str, Any] = {
        "name": name,
        "website": website,
        "international_email": crawled_data.get("international_email"),
        "international_phone": crawled_data.get("international_phone"),
        "esl_program": crawled_data.get("esl_program"),
        "international_support": crawled_data.get("international_support"),
        "facilities": crawled_data.get("facilities"),
    }

    if seed_school:
        payload.update(
            {
                "type": seed_school.get("type"),
                "state": seed_school.get("state"),
                "city": seed_school.get("city"),
                "tuition": seed_school.get("tuition"),
                "description": seed_school.get("description"),
            }
        )
    elif existing_school:
        payload["type"] = existing_school.type

    # None ê°’ì€ ì—…ë°ì´íŠ¸ ì‹œ ê¸°ì¡´ ìœ íš¨ê°’ì„ ë®ì–´ì“°ì§€ ì•Šë„ë¡ ì œì™¸
    return {k: v for k, v in payload.items() if v is not None}


def _record_crawl_audit(
    status: str,
    name: str,
    website: str,
    school_id: Optional[uuid.UUID] = None,
    extra: Optional[Dict[str, Any]] = None,
) -> None:
    """í¬ë¡¤ë§ ê°ì‚¬ ë¡œê·¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."""
    try:
        with get_db() as db:
            audit_user_id = _resolve_audit_user_id(db)
            if audit_user_id is None:
                # ìš´ì˜ DB ì œì•½ ë•Œë¬¸ì— audit ì €ì¥ì´ ì‹¤íŒ¨í•´ë„ í¬ë¡¤ë§ ìì²´ê°€ ê³„ì† ëŒì•„ê°€ì•¼ í•©ë‹ˆë‹¤.
                logger.error("AuditLog user_id í•´ì„ ì‹¤íŒ¨ë¡œ audit ê¸°ë¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return

            resolved_school_id = school_id
            if resolved_school_id is None:
                school = _find_school_record(db, name, website)
                resolved_school_id = school.id if school else uuid.uuid4()

            new_value: Dict[str, Any] = {
                # ìš´ì˜ DBì˜ audit_logs.action ì²´í¬ ì œì•½ì´ CRAWLì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆì–´,
                # í¬ë¡¤ë§ ì´ë²¤íŠ¸ë¥¼ new_valueë¡œ ì‹ë³„ ê°€ëŠ¥í•˜ê²Œ íƒœê¹…í•©ë‹ˆë‹¤.
                "event_type": "crawl",
                "status": status,
                "school_name": name,
                "website": website,
                "message": "í¬ë¡¤ë§ ì™„ë£Œ" if status == "success" else "í¬ë¡¤ë§ ì‹¤íŒ¨",
            }
            if extra:
                new_value.update(extra)
            # ê³¼ê±° í¬ë§· í˜¸í™˜: error_messageë§Œ ìˆëŠ” ê²½ìš° messageë¡œ ë³´ì •
            if not new_value.get("message") and new_value.get("error_message"):
                new_value["message"] = new_value["error_message"]

            db.add(
                AuditLog(
                    table_name="schools",
                    record_id=resolved_school_id,
                    # NOTE: ìš´ì˜ DB ì œì•½ í˜¸í™˜ì„ ìœ„í•´ UPDATEë¡œ ì €ì¥í•©ë‹ˆë‹¤.
                    # ëª¨ë‹ˆí„°ëŠ” (action=UPDATE && new_value.event_type=crawl) ë˜ëŠ” (action=CRAWL)ì„ ëª¨ë‘ ì¸ì‹í•©ë‹ˆë‹¤.
                    action="UPDATE",
                    new_value=new_value,
                    ip_address="crawler-system",
                    # ìš´ì˜ DB ì œì•½ ëŒ€ì‘: NOT NULL ë° FKë¥¼ ë§Œì¡±í•˜ëŠ” users.idë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.
                    user_id=audit_user_id,
                    created_by=audit_user_id,
                    updated_by=audit_user_id,
                )
            )
    except Exception as e:
        logger.error(f"AuditLog ê¸°ë¡ ì‹¤íŒ¨: {name} - {e}")


def crawl_single_school(
    name: str,
    website: str,
    seed_school: Optional[Dict[str, Any]] = None,
) -> dict:
    """
    ë‹¨ì¼ í•™êµ í¬ë¡¤ë§
    
    Args:
        name: í•™êµ ì´ë¦„
        website: ì›¹ì‚¬ì´íŠ¸ URL
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {name}")
    logger.info(f"{'='*60}\n")
    
    result = {
        "success": False,
        "ssl_error_detected": False,
        "ssl_error_message": "",
        "ssl_error_url": "",
        "school_id": None,
    }

    try:
        with SchoolCrawler(name, website) as crawler:
            data = crawler.crawl_all()
            result["ssl_error_detected"] = crawler.ssl_error_detected
            result["ssl_error_message"] = crawler.ssl_error_message
            result["ssl_error_url"] = crawler.ssl_error_url

            if crawler.ssl_error_detected:
                logger.warning(f"SSL ê²€ì¦ ì‹¤íŒ¨ë¡œ ì €ì¥ì„ ê±´ë„ˆëœ€: {name}")
                return result

            crawled = data.get("crawled_data", {})
            try:
                with get_db() as db:
                    repo = SchoolRepository(db)
                    existing_school = _find_school_record(db, name, website)

                    # Level 1 ë©”íƒ€ë°ì´í„° ë³´ê°•: College Scorecard API (ì‹¤íŒ¨í•´ë„ í¬ë¡¤ë§ì€ ê³„ì†)
                    state_hint = None
                    city_hint = None
                    if seed_school:
                        state_hint = seed_school.get("state") or None
                        city_hint = seed_school.get("city") or None
                    if existing_school:
                        state_hint = state_hint or getattr(existing_school, "state", None)
                        city_hint = city_hint or getattr(existing_school, "city", None)

                    scorecard_update, scorecard_audit = _SCORECARD_SERVICE.enrich_school(
                        school_name=name,
                        state=state_hint,
                        city=city_hint,
                    )
                    school_payload = _build_school_payload(
                        name=name,
                        website=website,
                        crawled_data=crawled,
                        seed_school=seed_school,
                        existing_school=existing_school,
                    )
                    # None ë®ì–´ì“°ê¸° ë°©ì§€: scorecard_updateëŠ” ê°’ì´ ìˆëŠ” í•„ë“œë§Œ í¬í•¨í•©ë‹ˆë‹¤.
                    school_payload.update(scorecard_update)

                    saved_school: Optional[School] = None
                    data_changed = False
                    if existing_school:
                        for key, value in school_payload.items():
                            if getattr(existing_school, key, None) != value:
                                data_changed = True
                                break
                        repo.update(existing_school.id, school_payload)
                        saved_school = existing_school
                        logger.info(f"DB ì—…ë°ì´íŠ¸ ì™„ë£Œ: {name}")
                    else:
                        if not school_payload.get("type"):
                            logger.warning(
                                f"DB ì €ì¥ ê±´ë„ˆëœ€(í•„ìˆ˜ í•„ë“œ type ì—†ìŒ): {name}"
                            )
                        else:
                            saved_school = repo.create(school_payload)
                            data_changed = True
                            logger.info(f"DB ìƒì„± ì™„ë£Œ: {name}")

                    if saved_school:
                        now = datetime.now(timezone.utc)
                        saved_school.last_crawled_at = now
                        saved_school.last_crawl_status = "success"
                        saved_school.last_crawl_message = "í¬ë¡¤ë§ ì™„ë£Œ"
                        if data_changed:
                            saved_school.last_crawl_data_updated_at = now
                        db.flush()

                        result["school_id"] = str(saved_school.id)
                        _record_crawl_audit(
                            status="success",
                            name=name,
                            website=website,
                            school_id=saved_school.id,
                            extra={
                                "data_summary": {
                                    "email": crawled.get("international_email", "N/A"),
                                    "phone": crawled.get("international_phone", "N/A"),
                                    "esl": crawled.get("esl_program", {}).get(
                                        "available", False
                                    ),
                                    "majors_count": len(crawled.get("majors", [])),
                                },
                                "enrichment": scorecard_audit,
                            },
                        )
                    else:
                        _record_crawl_audit(
                            status="success",
                            name=name,
                            website=website,
                            extra={
                                "data_summary": {
                                    "email": crawled.get("international_email", "N/A"),
                                    "phone": crawled.get("international_phone", "N/A"),
                                    "esl": crawled.get("esl_program", {}).get(
                                        "available", False
                                    ),
                                    "majors_count": len(crawled.get("majors", [])),
                                },
                                "enrichment": scorecard_audit,
                                "note": "DB row ì—†ì´ í¬ë¡¤ë§ ì„±ê³µ ë¡œê·¸ë§Œ ê¸°ë¡",
                            },
                        )
            except Exception as e:
                logger.error(f"DB ì €ì¥ ì‹¤íŒ¨: {name} - {e}")

            result["success"] = True
            
            # ìš”ì•½ ì¶œë ¥
            logger.info(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
            logger.info(f"  - ì´ë©”ì¼: {crawled.get('international_email', 'N/A')}")
            logger.info(f"  - ì „í™”: {crawled.get('international_phone', 'N/A')}")
            logger.info(f"  - ESL: {crawled.get('esl_program', {}).get('available', False)}")
            logger.info(f"  - ì „ê³µ ìˆ˜: {len(crawled.get('majors', []))}")
            
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    return result


def crawl_all_schools(json_file: Path, limit: int = None) -> None:
    """
    ëª¨ë“  í•™êµ í¬ë¡¤ë§
    
    Args:
        json_file: í•™êµ ëª©ë¡ JSON íŒŒì¼
        limit: í¬ë¡¤ë§í•  ìµœëŒ€ í•™êµ ìˆ˜ (Noneì´ë©´ ì „ì²´)
    """
    schools = load_schools_list(json_file)
    
    if limit:
        schools = schools[:limit]
    
    logger.info(f"ğŸ“š ì´ {len(schools)}ê°œ í•™êµ í¬ë¡¤ë§ ì‹œì‘\n")
    
    success_count = 0
    fail_count = 0
    
    for i, school in enumerate(schools, 1):
        name = school.get('name')
        website = school.get('website')
        
        if not name or not website:
            logger.warning(f"â­ï¸  ê±´ë„ˆëœ€: ì •ë³´ ë¶€ì¡± - {school}")
            fail_count += 1
            continue

        should_skip, skip_reason = failed_site_manager.should_skip(website)
        if should_skip:
            logger.warning(f"â­ï¸  ê±´ë„ˆëœ€: {name} - {skip_reason}")
            _record_crawl_audit(
                status="failed",
                name=name,
                website=website,
                extra={
                    "message": f"ê±´ë„ˆëœ€: {skip_reason}",
                    "error_type": "skip_failed_site",
                    "error_message": skip_reason,
                },
            )
            _update_school_crawl_metadata(
                name=name,
                website=website,
                status="skipped",
                message=f"ê±´ë„ˆëœ€: {skip_reason}",
            )
            fail_count += 1
            continue
        
        logger.info(f"\n[{i}/{len(schools)}] {name}")
        
        try:
            result = crawl_single_school(name, website, seed_school=school)
            if result.get("ssl_error_detected", False):
                failed_site_manager.add_ssl_failure(
                    name=name,
                    website=website,
                    error_message=result.get("ssl_error_message", "SSL verification failed"),
                    note=f"ë§ˆì§€ë§‰ ì‹¤íŒ¨ URL: {result.get('ssl_error_url', website)}",
                )
                logger.warning(f"â­ï¸  SSL ì‹¤íŒ¨ ì‚¬ì´íŠ¸ë¡œ ê¸°ë¡: {name}")
                school_id = result.get("school_id")
                _record_crawl_audit(
                    status="failed",
                    name=name,
                    website=website,
                    school_id=uuid.UUID(school_id) if school_id else None,
                    extra={
                        "message": "SSL ê²€ì¦ ì‹¤íŒ¨ë¡œ í¬ë¡¤ë§ ì¤‘ë‹¨",
                        "error_type": "ssl_verification",
                        "error_message": result.get("ssl_error_message", ""),
                    },
                )
                _update_school_crawl_metadata(
                    name=name,
                    website=website,
                    status="failed",
                    message="SSL ê²€ì¦ ì‹¤íŒ¨ë¡œ í¬ë¡¤ë§ ì¤‘ë‹¨",
                )
                fail_count += 1
            elif result.get("success", False):
                success_count += 1
            else:
                school_id = result.get("school_id")
                _record_crawl_audit(
                    status="failed",
                    name=name,
                    website=website,
                    school_id=uuid.UUID(school_id) if school_id else None,
                    extra={
                        "message": "í¬ë¡¤ë§ ì²˜ë¦¬ ì‹¤íŒ¨",
                        "error_type": "crawl_failed",
                        "error_message": "í¬ë¡¤ë§ ì²˜ë¦¬ ì‹¤íŒ¨",
                    },
                )
                _update_school_crawl_metadata(
                    name=name,
                    website=website,
                    status="failed",
                    message="í¬ë¡¤ë§ ì²˜ë¦¬ ì‹¤íŒ¨",
                )
                fail_count += 1
        except Exception as e:
            logger.error(f"âŒ ì‹¤íŒ¨: {e}")
            _record_crawl_audit(
                status="failed",
                name=name,
                website=website,
                extra={
                    "message": "ì˜ˆì™¸ ë°œìƒìœ¼ë¡œ í¬ë¡¤ë§ ì‹¤íŒ¨",
                    "error_type": "exception",
                    "error_message": str(e),
                },
            )
            _update_school_crawl_metadata(
                name=name,
                website=website,
                status="failed",
                message="ì˜ˆì™¸ ë°œìƒìœ¼ë¡œ í¬ë¡¤ë§ ì‹¤íŒ¨",
            )
            fail_count += 1
    
    # ìµœì¢… ê²°ê³¼
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼")
    logger.info(f"{'='*60}")
    logger.info(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    logger.info(f"âŒ ì‹¤íŒ¨: {fail_count}ê°œ")
    logger.info("ğŸ’¾ ì €ì¥ ë°©ì‹: DB ë‹¨ì¼ ì €ì¥")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='College Crawler - ë¯¸êµ­ ëŒ€í•™ ì •ë³´ ìˆ˜ì§‘')
    
    parser.add_argument('command', choices=['crawl', 'test'], 
                       help='ì‹¤í–‰í•  ëª…ë ¹ (crawl: í¬ë¡¤ë§ ì‹¤í–‰, test: í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§)')
    parser.add_argument('--school', type=str, 
                       help='í¬ë¡¤ë§í•  íŠ¹ì • í•™êµ ì´ë¦„')
    parser.add_argument('--website', type=str, 
                       help='í•™êµ ì›¹ì‚¬ì´íŠ¸ URL (--schoolê³¼ í•¨ê»˜ ì‚¬ìš©)')
    parser.add_argument('--limit', type=int, 
                       help='í¬ë¡¤ë§í•  ìµœëŒ€ í•™êµ ìˆ˜')
    
    args = parser.parse_args()
    
    project_root = Path(__file__).parent.parent
    
    if args.command == 'test':
        # í…ŒìŠ¤íŠ¸: ì²« ë²ˆì§¸ í•™êµë§Œ í¬ë¡¤ë§
        logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì²« ë²ˆì§¸ í•™êµë§Œ í¬ë¡¤ë§")
        json_file = project_root / 'data' / 'schools_initial.json'
        crawl_all_schools(json_file, limit=1)
        
    elif args.command == 'crawl':
        if args.school and args.website:
            # íŠ¹ì • í•™êµ í¬ë¡¤ë§
            crawl_single_school(args.school, args.website)
        else:
            # ì „ì²´ í•™êµ í¬ë¡¤ë§
            json_file = project_root / 'data' / 'schools_initial.json'
            crawl_all_schools(json_file, limit=args.limit)


if __name__ == '__main__':
    main()
