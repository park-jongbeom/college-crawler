"""
College Crawler ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""

import argparse
import json
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.crawlers.school_crawler import SchoolCrawler
from src.utils.logger import setup_logger

logger = setup_logger(__name__)


def load_schools_list(json_file: Path) -> list:
    """
    í•™êµ ëª©ë¡ JSON íŒŒì¼ ë¡œë“œ
    
    Args:
        json_file: JSON íŒŒì¼ ê²½ë¡œ
        
    Returns:
        í•™êµ ëª©ë¡
    """
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get('schools', [])


def crawl_single_school(name: str, website: str, output_dir: Path) -> None:
    """
    ë‹¨ì¼ í•™êµ í¬ë¡¤ë§
    
    Args:
        name: í•™êµ ì´ë¦„
        website: ì›¹ì‚¬ì´íŠ¸ URL
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {name}")
    logger.info(f"{'='*60}\n")
    
    try:
        with SchoolCrawler(name, website) as crawler:
            data = crawler.crawl_all()
            crawler.save_to_json(output_dir)
            
            # ìš”ì•½ ì¶œë ¥
            crawled = data.get('crawled_data', {})
            logger.info(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
            logger.info(f"  - ì´ë©”ì¼: {crawled.get('international_email', 'N/A')}")
            logger.info(f"  - ì „í™”: {crawled.get('international_phone', 'N/A')}")
            logger.info(f"  - ESL: {crawled.get('esl_program', {}).get('available', False)}")
            logger.info(f"  - ì „ê³µ ìˆ˜: {len(crawled.get('majors', []))}")
            
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")


def crawl_all_schools(json_file: Path, output_dir: Path, limit: int = None) -> None:
    """
    ëª¨ë“  í•™êµ í¬ë¡¤ë§
    
    Args:
        json_file: í•™êµ ëª©ë¡ JSON íŒŒì¼
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        limit: í¬ë¡¤ë§í•  ìµœëŒ€ í•™êµ ìˆ˜ (Noneì´ë©´ ì „ì²´)
    """
    schools = load_schools_list(json_file)
    
    if limit:
        schools = schools[:limit]
    
    logger.info(f"ğŸ“š ì´ {len(schools)}ê°œ í•™êµ í¬ë¡¤ë§ ì‹œì‘\n")
    
    success_count = 0
    fail_count = 0
    
    for i, school in enumerate(schools, 1):
        name = school.get('name')
        website = school.get('website')
        
        if not name or not website:
            logger.warning(f"â­ï¸  ê±´ë„ˆëœ€: ì •ë³´ ë¶€ì¡± - {school}")
            fail_count += 1
            continue
        
        logger.info(f"\n[{i}/{len(schools)}] {name}")
        
        try:
            crawl_single_school(name, website, output_dir)
            success_count += 1
        except Exception as e:
            logger.error(f"âŒ ì‹¤íŒ¨: {e}")
            fail_count += 1
    
    # ìµœì¢… ê²°ê³¼
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼")
    logger.info(f"{'='*60}")
    logger.info(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    logger.info(f"âŒ ì‹¤íŒ¨: {fail_count}ê°œ")
    logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir.absolute()}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='College Crawler - ë¯¸êµ­ ëŒ€í•™ ì •ë³´ ìˆ˜ì§‘')
    
    parser.add_argument('command', choices=['crawl', 'test'], 
                       help='ì‹¤í–‰í•  ëª…ë ¹ (crawl: í¬ë¡¤ë§ ì‹¤í–‰, test: í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§)')
    parser.add_argument('--school', type=str, 
                       help='í¬ë¡¤ë§í•  íŠ¹ì • í•™êµ ì´ë¦„')
    parser.add_argument('--website', type=str, 
                       help='í•™êµ ì›¹ì‚¬ì´íŠ¸ URL (--schoolê³¼ í•¨ê»˜ ì‚¬ìš©)')
    parser.add_argument('--limit', type=int, 
                       help='í¬ë¡¤ë§í•  ìµœëŒ€ í•™êµ ìˆ˜')
    parser.add_argument('--output', type=str, default='data/crawled',
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data/crawled)')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    project_root = Path(__file__).parent.parent
    output_dir = project_root / args.output
    
    if args.command == 'test':
        # í…ŒìŠ¤íŠ¸: ì²« ë²ˆì§¸ í•™êµë§Œ í¬ë¡¤ë§
        logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì²« ë²ˆì§¸ í•™êµë§Œ í¬ë¡¤ë§")
        json_file = project_root / 'data' / 'schools_initial.json'
        crawl_all_schools(json_file, output_dir, limit=1)
        
    elif args.command == 'crawl':
        if args.school and args.website:
            # íŠ¹ì • í•™êµ í¬ë¡¤ë§
            crawl_single_school(args.school, args.website, output_dir)
        else:
            # ì „ì²´ í•™êµ í¬ë¡¤ë§
            json_file = project_root / 'data' / 'schools_initial.json'
            crawl_all_schools(json_file, output_dir, limit=args.limit)


if __name__ == '__main__':
    main()
