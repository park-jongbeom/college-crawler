"""
í•™êµ ì •ë³´ í¬ë¡¤ëŸ¬
"""

import json
from typing import Dict, Any, Optional
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.crawlers.base_crawler import BaseCrawler
from src.crawlers.parsers.contact_parser import ContactParser
from src.crawlers.parsers.facility_parser import FacilityParser
from src.crawlers.parsers.program_parser import ProgramParser
from src.utils.logger import setup_logger

logger = setup_logger(__name__)


class SchoolCrawler(BaseCrawler):
    """í•™êµ ì •ë³´ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, school_name: str, website: str):
        """
        ì´ˆê¸°í™”
        
        Args:
            school_name: í•™êµ ì´ë¦„
            website: í•™êµ ì›¹ì‚¬ì´íŠ¸ URL
        """
        super().__init__(website)
        self.school_name = school_name
        self.data: Dict[str, Any] = {
            'name': school_name,
            'website': website,
            'crawled_data': {}
        }
    
    def crawl_all(self) -> Dict[str, Any]:
        """
        ëª¨ë“  ì •ë³´ í¬ë¡¤ë§
        
        Returns:
            í¬ë¡¤ë§ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"=== {self.school_name} í¬ë¡¤ë§ ì‹œì‘ ===")
        
        try:
            # 1. ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
            self._crawl_homepage()
            
            # 2. International Students í˜ì´ì§€ í¬ë¡¤ë§
            self._crawl_international_page()
            
            # 3. Programs í˜ì´ì§€ í¬ë¡¤ë§
            self._crawl_programs_page()
            
            # 4. Campus Life í˜ì´ì§€ í¬ë¡¤ë§
            self._crawl_campus_life_page()
            
            logger.info(f"âœ… {self.school_name} í¬ë¡¤ë§ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ {self.school_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        
        return self.data
    
    def _crawl_homepage(self) -> None:
        """ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§"""
        logger.info(f"ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§: {self.base_url}")
        
        response = self.fetch(self.base_url)
        if not response:
            logger.warning("ë©”ì¸ í˜ì´ì§€ ì‘ë‹µ ì—†ìŒ")
            return
        
        html = response.text
        
        # ê¸°ë³¸ ì—°ë½ì²˜ ì •ë³´ íŒŒì‹±
        contact_info = ContactParser.parse_contact_info(html)
        self.data['crawled_data'].update(contact_info)
        
        logger.info(f"ë©”ì¸ í˜ì´ì§€ íŒŒì‹± ì™„ë£Œ: {contact_info}")
    
    def _crawl_international_page(self) -> None:
        """International Students í˜ì´ì§€ í¬ë¡¤ë§"""
        # International ê´€ë ¨ ê°€ëŠ¥í•œ URL íŒ¨í„´
        patterns = [
            '/international',
            '/international-students',
            '/admissions/international',
            '/students/international',
            '/global',
            '/international-programs'
        ]
        
        for pattern in patterns:
            url = self.get_absolute_url(pattern)
            logger.info(f"International í˜ì´ì§€ ì‹œë„: {url}")
            
            response = self.fetch(url)
            if response and response.status_code == 200:
                html = response.text
                
                # ì—°ë½ì²˜ ì¬íŒŒì‹± (ë” ì •í™•í•œ ì •ë³´ ê°€ëŠ¥)
                contact_info = ContactParser.parse_contact_info(html)
                if contact_info.get('international_email'):
                    self.data['crawled_data'].update(contact_info)
                
                # ìœ í•™ìƒ ì§€ì› ì •ë³´
                support_info = ProgramParser.parse_international_support(html)
                self.data['crawled_data']['international_support'] = support_info
                
                # ESL í”„ë¡œê·¸ë¨
                esl_info = ProgramParser.parse_esl_program(html)
                self.data['crawled_data']['esl_program'] = esl_info
                
                logger.info(f"âœ… International í˜ì´ì§€ íŒŒì‹± ì™„ë£Œ")
                break
        else:
            logger.warning("International í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    def _crawl_programs_page(self) -> None:
        """Programs/Academics í˜ì´ì§€ í¬ë¡¤ë§"""
        patterns = [
            '/programs',
            '/academics',
            '/academics/programs',
            '/degrees',
            '/programs-of-study'
        ]
        
        for pattern in patterns:
            url = self.get_absolute_url(pattern)
            logger.info(f"Programs í˜ì´ì§€ ì‹œë„: {url}")
            
            response = self.fetch(url)
            if response and response.status_code == 200:
                html = response.text
                
                # ì „ê³µ ëª©ë¡
                majors = ProgramParser.parse_majors(html)
                if majors:
                    self.data['crawled_data']['majors'] = majors
                
                # ESL í”„ë¡œê·¸ë¨ (ì•„ì§ ì—†ìœ¼ë©´)
                if 'esl_program' not in self.data['crawled_data']:
                    esl_info = ProgramParser.parse_esl_program(html)
                    self.data['crawled_data']['esl_program'] = esl_info
                
                logger.info(f"âœ… Programs í˜ì´ì§€ íŒŒì‹± ì™„ë£Œ: {len(majors)}ê°œ ì „ê³µ")
                break
        else:
            logger.warning("Programs í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    def _crawl_campus_life_page(self) -> None:
        """Campus Life/Facilities í˜ì´ì§€ í¬ë¡¤ë§"""
        patterns = [
            '/campus-life',
            '/student-life',
            '/campus',
            '/facilities',
            '/about/campus'
        ]
        
        for pattern in patterns:
            url = self.get_absolute_url(pattern)
            logger.info(f"Campus Life í˜ì´ì§€ ì‹œë„: {url}")
            
            response = self.fetch(url)
            if response and response.status_code == 200:
                html = response.text
                
                # ì‹œì„¤ ì •ë³´
                facilities = FacilityParser.parse_facilities(html)
                self.data['crawled_data']['facilities'] = facilities
                
                # ì‹œì„¤ ìƒì„¸
                facility_details = FacilityParser.parse_facility_details(html)
                if facility_details:
                    self.data['crawled_data']['facility_details'] = facility_details
                
                logger.info(f"âœ… Campus Life í˜ì´ì§€ íŒŒì‹± ì™„ë£Œ")
                break
        else:
            logger.warning("Campus Life í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    def save_to_json(self, output_dir: Path) -> None:
        """
        í¬ë¡¤ë§ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª…: í•™êµ ì´ë¦„ (ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ)
        filename = self.school_name.replace(' ', '_').replace('â€“', '-') + '.json'
        filepath = output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {filepath}")
