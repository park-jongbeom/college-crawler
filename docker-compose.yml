# Production Docker Compose Configuration
# Environment variables should be configured in .env file (see .env.example)

services:
  college-crawler:
    image: patrick5471/college-crawler:latest
    container_name: college-crawler
    env_file:
      - .env
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
    volumes:
      - crawler_data:/app/data/crawled
      - crawler_logs:/app/logs
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker 소켓 마운트 (모니터링용)
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import sys; sys.exit(0)' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - crawler-network
    # 주기적 크롤링: 매일 새벽 2시에 자동 실행
    command: 
      - bash
      - -c
      - |
        # Install cron for Debian-based image
        apt-get update && apt-get install -y cron && rm -rf /var/lib/apt/lists/*
        # Setup cron job: daily at 2 AM
        echo "0 2 * * * cd /app && python src/main.py crawl >> /app/logs/cron.log 2>&1" | crontab -
        # Start cron in foreground
        cron && tail -f /dev/null

  monitor:
    image: patrick5471/college-crawler:latest
    container_name: college-crawler-monitor
    env_file:
      - .env
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
      - LOG_FILE=/tmp/monitor.log  # 임시 디렉토리에 로그 저장
    ports:
      - "8081:8080"
    volumes:
      - crawler_data:/app/data/crawled  # crawler와 failed_sites.json 공유
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker 접근 권한
    group_add:
      - "114"  # docker 그룹 GID (호스트의 docker 소켓 그룹)
    depends_on:
      - college-crawler
    restart: unless-stopped
    networks:
      - crawler-network
    command: ["python", "-m", "uvicorn", "src.monitor.api:app", "--host", "0.0.0.0", "--port", "8080"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

volumes:
  crawler_data:
    driver: local
  crawler_logs:
    driver: local

networks:
  crawler-network:
    driver: bridge
