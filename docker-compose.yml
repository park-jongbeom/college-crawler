# Production Docker Compose Configuration
# Environment variables should be configured in .env file (see .env.example)

services:
  college-crawler:
    image: patrick5471/college-crawler:latest
    container_name: college-crawler
    # cron 데몬 구동을 위해 root로 실행하되, 실제 크롤링은 crawler 유저로 실행합니다.
    user: root
    env_file:
      - .env
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
    volumes:
      - crawler_data:/app/data/crawled
      - crawler_logs:/app/logs
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker 소켓 마운트 (모니터링용)
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import sys; sys.exit(0)' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - crawler-network
    # 주기적 크롤링: 매일 새벽 2시에 자동 실행
    command: 
      - bash
      - -c
      - |
        # Dockerfile에서 cron/bash를 사전 설치합니다. (여기서 apt-get 금지)
        # CRON_TZ로 컨테이너/호스트 TZ와 무관하게 KST 새벽 2시 실행을 보장합니다.
        cat > /tmp/crontab <<'EOF'
        CRON_TZ=Asia/Seoul
        0 2 * * * su -s /bin/bash crawler -c 'cd /app && python src/main.py crawl >> /app/logs/cron.log 2>&1'
        EOF
        crontab /tmp/crontab

        # cron 데몬 실행 (foreground 옵션이 없는 경우도 있어 daemonize 후 유지)
        cron
        tail -f /dev/null

  monitor:
    image: patrick5471/college-crawler:latest
    container_name: college-crawler-monitor
    env_file:
      - .env
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
      - LOG_FILE=/tmp/monitor.log  # 임시 디렉토리에 로그 저장
    ports:
      - "8081:8080"
    volumes:
      - crawler_data:/app/data/crawled  # crawler와 failed_sites.json 공유
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker 접근 권한
    group_add:
      - "114"  # docker 그룹 GID (호스트의 docker 소켓 그룹)
    depends_on:
      - college-crawler
    restart: unless-stopped
    networks:
      - crawler-network
    command: ["python", "-m", "uvicorn", "src.monitor.api:app", "--host", "0.0.0.0", "--port", "8080"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

volumes:
  crawler_data:
    driver: local
  crawler_logs:
    driver: local

networks:
  crawler-network:
    driver: bridge
