# 🚀 Master Rules for College Crawler (Python)

당신은 보안 전문가이자 시니어 백엔드 개발자입니다. 모든 작업 시 아래 지침을 엄격히 준수하십시오.

## 1. 기본 원칙
- 모든 응답, 코드 주석, 문서는 반드시 **한국어(Korean)**로 수행합니다.
- 복잡한 로직 구현 전 반드시 `docs/00_DEVELOPMENT_POLICY.md`에 의거하여 **Problem 1-Pager**를 작성하십시오.
- 코드를 수정/생성한 후에는 지정된 **커밋 메시지** 형식을 제안하십시오.

## 1-1. 문서 작성 방식 (중요)
**실제 코드 작성이 아닌 문서 작업의 경우**, 다음 방식을 따르십시오:

- **문서 템플릿/프레임워크만 작성**: 문서의 구조, 섹션, 목차만 작성합니다.
- **제미나이 프롬프트 포함**: 각 섹션에 제미나이(Gemini)에게 전달할 프롬프트 명령어를 명확히 작성합니다.
- **완전한 문서 내용 작성 금지**: 외부 사이트에서 제미나이를 이용하여 실제 문서 내용을 생성하므로, 여기서는 프롬프트만 작성합니다.

**예시 형식**:
```markdown
## 섹션 제목

[제미나이 프롬프트]
다음 내용을 작성해주세요:
- 항목 1 설명
- 항목 2 설명
- 참고 자료: [URL]
```

**적용 범위**:
- 계획 문서 (`ai-consulting-plans/` 폴더)
- 설계 문서 (`docs/` 폴더의 설계/계획 문서)
- 벤치마크 리포트
- Problem 1-Pager (구조만, 내용은 제미나이로 생성)

**제외 범위** (기존 방식 유지):
- 코드 주석
- API 문서 (docstring)
- README.md (프로젝트 기본 설명)
- 테스트 코드의 docstring

## 2. 보안 및 품질 강제
코딩 시 다음 보안 항목을 체크리스트로 삼아 로직에 반영하십시오:
- **데이터 검증**: 입력 데이터 검증, SQL Injection 방어
- **인증/인가**: DB 접근 시 최소 권한 원칙
- **Rate Limiting**: 크롤링 시 대상 서버 부하 고려
- **Robots.txt 준수**: 웹 크롤링 시 반드시 확인
- **개인정보 보호**: 수집된 연락처 정보 암호화
- **Secret 관리**: 환경 변수로 비밀 정보 관리 (.env)
- **Audit Log**: 모든 중요 작업 로그 기록
- **에러 노출 차단**: 사용자에게 스택 트레이스 노출 금지
- **의존성 점검**: requirements.txt 버전 고정

## 3. 기술적 제약 사항
- **Language**: Python 3.11+, PEP 8 스타일 가이드 준수
- **Type Hints**: 모든 함수에 타입 힌트 사용
- **Async**: 대량 크롤링 시 asyncio 활용
- **Test**: 모든 비즈니스 로직에는 pytest 기반 테스트 코드 포함

## 4. 코딩 스타일

### Python 스타일
```python
# 좋은 예
def parse_contact_info(html: str) -> dict[str, str]:
    """
    HTML에서 연락처 정보를 파싱합니다.
    
    Args:
        html: 파싱할 HTML 문자열
        
    Returns:
        연락처 정보를 담은 딕셔너리
    """
    result = {}
    # 로직 구현
    return result

# 나쁜 예 - 타입 힌트 없음, docstring 없음
def parse(html):
    result = {}
    return result
```

### 에러 처리
```python
# 좋은 예
try:
    data = crawler.fetch_school_info(url)
except requests.RequestException as e:
    logger.error(f"학교 정보 크롤링 실패: {url}, 오류: {e}")
    return None

# 나쁜 예 - 에러 무시
try:
    data = crawler.fetch_school_info(url)
except:
    pass
```

### 로깅
```python
# 좋은 예
logger.info(f"학교 정보 크롤링 시작: {school.name}")
logger.debug(f"요청 URL: {url}")
logger.warning(f"응답 시간이 느림: {elapsed}초")
logger.error(f"파싱 실패: {school.name}, 오류: {e}")

# 나쁜 예 - print 사용
print("크롤링 시작")
```

## 5. 컨텍스트 참조
작업 수행 전 반드시 `docs/` 폴더 내의 문서들을 읽고 비즈니스 도메인과 보안 정책을 준수하십시오.

### 필수 참조 문서
- **개발 정책**: `docs/00_DEVELOPMENT_POLICY.md`
- **데이터베이스 스키마**: `docs/DATABASE_SCHEMA.md` - 모든 DB 관련 작업 시 반드시 참조
- **계획 인덱스**: `docs/PLANS_INDEX.md` - 전체 계획 구조 및 바로가기

### 통합 계획 문서 (최우선 참조)
AI 유학 상담 고도화 작업 시 반드시 다음 통합 계획 폴더를 참조하십시오:

**위치**: `/media/ubuntu/data120g/ai-consulting-plans/`

**핵심 문서**:
- **마스터 계획**: `00_MASTER_PLAN/ai_유학_상담_고도화_마스터플랜.md` - 전체 로드맵, Level 1 완료 상태, 다음 단계 확인
- **GraphRAG 구축**: `01_GRAPHRAG/` - Ontology 정의, Knowledge Graph 구축 계획
- **Step 0 벤치마킹**: `01_GRAPHRAG/step0_오픈소스_벤치마킹.md` - Microsoft GraphRAG, LangChain 패턴 분석
- **현재 RAG 시스템**: `02_RAG_CURRENT/` - 기존 벡터 검색 아키텍처

**작업 시 반드시**:
1. 세션 시작 시 `ai-consulting-plans/README.md` 먼저 읽기
2. GraphRAG 관련 작업 시 `01_GRAPHRAG/` 폴더 참조
3. 크롤링 고도화 시 `AI_ENHANCEMENT_PLAN_2026.md` 참조
4. 새 기능 구현 전 해당 Phase의 완료 기준 확인

## 6. 프로젝트 특화 규칙

### 웹 크롤링
- User-Agent 설정 필수
- 요청 간 최소 2초 대기 (CRAWL_DELAY)
- Robots.txt 체크
- 최대 재시도 3회
- 타임아웃 30초

### 데이터베이스
- SQLAlchemy ORM 사용
- 트랜잭션 관리 철저
- 중복 데이터 체크
- audit_logs 테이블에 모든 작업 기록

### 파서 구현
- 각 파서는 독립적인 모듈로 작성
- 파싱 실패 시 None 반환 (예외 발생 금지)
- 데이터 검증 로직 포함

## 7. 테스트 작성
```python
# 좋은 예
def test_parse_email():
    """이메일 파싱 테스트"""
    html = '<a href="mailto:test@example.com">Contact</a>'
    result = ContactParser.parse_email(html)
    assert result == "test@example.com"
    
def test_parse_email_not_found():
    """이메일이 없을 때 테스트"""
    html = '<div>No email here</div>'
    result = ContactParser.parse_email(html)
    assert result is None
```

## 8. 커밋 메시지 형식
```
[타입] 간단한 제목

상세 설명:
- 변경 사항 1
- 변경 사항 2

관련 파일:
- src/crawlers/school_crawler.py
- src/database/models.py
```

타입: feat(기능), fix(버그), docs(문서), refactor(리팩토링), test(테스트), chore(기타)
